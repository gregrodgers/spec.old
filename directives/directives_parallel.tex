% This is an included file. See the master file for more information.
%
% When editing this file:
%
%    1. To change formatting, appearance, or style, please edit openmp.sty.
%
%    2. Custom commands and macros are defined in openmp.sty.
%
%    3. Be kind to other editors -- keep a consistent style by copying-and-pasting to
%       create new content.
%
%    4. We use semantic markup, e.g. (see openmp.sty for a full list):
%         \code{}     % for bold monospace keywords, code, operators, etc.
%         \plc{}      % for italic placeholder names, grammar, etc.
%
%    5. There are environments that provide special formatting, e.g. language bars.
%       Please use them whereever appropriate.  Examples are:
%
%         \begin{fortranspecific}
%         This is text that appears enclosed in blue language bars for Fortran.
%         \end{fortranspecific}
%
%         \begin{note}
%         This is a note.  The "Note -- " header appears automatically.
%         \end{note}
%
%    6. Other recommendations:
%         Use the convenience macros defined in openmp.sty for the minor headers
%         such as Comments, Syntax, etc.
%
%         To keep items together on the same page, prefer the use of
%         \begin{samepage}.... Avoid \parbox for text blocks as it interrupts line numbering.
%         When possible, avoid \filbreak, \pagebreak, \newpage, \clearpage unless that's
%         what you mean. Use \needspace{} cautiously for troublesome paragraphs.
%
%         Avoid absolute lengths and measures in this file; use relative units when possible.
%         Vertical space can be relative to \baselineskip or ex units. Horizontal space
%         can be relative to \linewidth or em units.
%
%         Prefer \emph{} to italicize terminology, e.g.:
%             This is a \emph{definition}, not a placeholder.
%             This is a \plc{var-name}.
%



\section{\hcode{parallel} Construct}
\index{parallel@{\code{parallel}}}
\index{constructs!parallel@{\code{parallel}}}
\label{sec:parallel Construct}
\summary
The parallel construct creates a team of OpenMP threads that execute the region.

\syntax
\begin{ccppspecific}
The syntax of the \code{parallel} construct is as follows:
\begin{ompcPragma}
#pragma omp parallel \plc{[clause[ [},\plc{] clause] ... ] new-line}
   \plc{structured-block}
\end{ompcPragma}

where \plc{clause} is one of the following:

{}
\begin{indentedcodelist}
if(\plc{[}parallel :\plc{] scalar-expression})
num_threads(\plc{integer-expression})
default(shared \textnormal{|} none)
private(\plc{list})
firstprivate(\plc{list})
shared(\plc{list})
copyin(\plc{list})
reduction(\plc{[ reduction-modifier},\plc{] reduction-identifier }:\plc{ list})
proc_bind(master \textnormal{|} close \textnormal{|} spread)
allocate(\plc{[allocator }:\plc{] list})
\end{indentedcodelist}
\end{ccppspecific}

\begin{fortranspecific}
The syntax of the \code{parallel} construct is as follows:

\begin{ompfPragma}
!$omp parallel \plc{[clause[ [},\plc{] clause] ... ]}
   \plc{structured-block}
!$omp end parallel
\end{ompfPragma}

\begin{samepage}
where \plc{clause} is one of the following:

{}
\begin{indentedcodelist}
if(\plc{[}parallel :\plc{] scalar-logical-expression})
num_threads(\plc{scalar-integer-expression})
default(private \textnormal{|} firstprivate \textnormal{|} shared \textnormal{|} none)
private(\plc{list})
firstprivate(\plc{list})
shared(\plc{list})
copyin(\plc{list})
reduction(\plc{[ reduction-modifier},\plc{] reduction-identifier }:\plc{ list})
proc_bind(master \textnormal{|} close \textnormal{|} spread)
allocate(\plc{[allocator }:\plc{] list})
\end{indentedcodelist}
\end{samepage}

\end{fortranspecific}

\binding
The binding thread set for a \code{parallel} region is the encountering thread. The
encountering thread becomes the master thread of the new team.

\descr
When a thread encounters a \code{parallel} construct, a team of threads is created to
execute the \code{parallel} region (see
\specref{subsec:Determining the Number of Threads for a parallel Region}
for more information about how the number of threads in the team is determined, 
including the evaluation of the \code{if} and \code{num_threads} clauses). The 
thread that encountered the \code{parallel} construct becomes the master thread 
of the new team, with a thread number of zero for the duration of the new 
\code{parallel} region. All threads in the new team, including the master thread, 
execute the region. Once the team is created, the number of threads in the
team remains constant for the duration of that \code{parallel} region.

The optional \code{proc_bind} clause, described in
\specref{subsec:Controlling OpenMP Thread Affinity}, specifies the mapping of 
OpenMP threads to places within the current place partition, that is, within
the places listed in the \plc{place-partition-var} ICV for the implicit task 
of the encountering thread.

Within a \code{parallel} region, thread numbers uniquely identify each thread. Thread
numbers are consecutive whole numbers ranging from zero for the master thread up to
one less than the number of threads in the team. A thread may obtain its own thread
number by a call to the \code{omp_get_thread_num} library routine.

A set of implicit tasks, equal in number to the number of threads in the team, is
generated by the encountering thread. The structured block of the \code{parallel} 
construct determines the code that will be executed in each implicit task. Each 
task is assigned to a different thread in the team and becomes tied. The task region 
of the task being executed by the encountering thread is suspended and each thread 
in the team executes its implicit task. Each thread can execute a path of statements 
that is different from that of the other threads.

The implementation may cause any thread to suspend execution of its implicit task at 
a task scheduling point, and to switch to execution of any explicit task generated by 
any of the threads in the team, before eventually resuming execution of the implicit 
task (for more details see \specref{sec:Tasking Constructs}).

There is an implied barrier at the end of a \code{parallel} region. After the end of a
\code{parallel} region, only the master thread of the team resumes execution of the
enclosing task region.

If a thread in a team executing a \code{parallel} region encounters another \code{parallel}
directive, it creates a new team, according to the rules in
\specref{subsec:Determining the Number of Threads for a parallel Region},
and it becomes the master of that new team.

If execution of a thread terminates while inside a \code{parallel} region, execution of all
threads in all teams terminates. The order of termination of threads is unspecified. All
work done by a team prior to any barrier that the team has passed in the program is
guaranteed to be complete. The amount of work done by each thread after the last
barrier that it passed and before it terminates is unspecified.

\events

The \plc{parallel-begin} event occurs in a thread that encounters a
\code{parallel} construct before any implicit task is created for the
corresponding \code{parallel} region.

Upon creation of each implicit task, an \plc{implicit-task-begin} event
occurs in the thread that executes the implicit task after the implicit
task is fully initialized but before the thread begins to execute the
structured block of the \code{parallel} construct.

If the \code{parallel} region creates a thread, a \plc{thread-begin}
event occurs as the first event in the context of the new thread
prior to the \plc{implicit-task-begin} event.

Events associated with implicit barriers occur at the end of a
\code{parallel} region. Section~\ref{subsec:implict-barrier} describes events
associated with implicit barriers.

When a thread finishes an implicit task, an \plc{implicit-task-end}
event occurs in the thread after events associated with implicit
barrier synchronization in the implicit task.

The \plc{parallel-end} event occurs in the thread that encounters the
\code{parallel} construct after the thread executes its \plc{implicit-task-end} 
event but before the thread resumes execution of the encountering task.

If a thread is destroyed at the end of a \code{parallel} region, a
\plc{thread-end} event occurs in the thread as the last event prior 
to destruction of the thread.

\tools

A thread dispatches a registered \code{ompt_callback_parallel_begin}
callback for each occurrence of a \plc{parallel-begin} event in that
thread.  The callback occurs in the task that encounters the 
\code{parallel} construct.  This callback has the type signature
\code{ompt_callback_parallel_begin_t}. In the dispatched callback, 
\code{(flag}~\code{&}~\code{ompt_parallel_team)} evaluates to \plc{true}.

A thread dispatches a registered \code{ompt_callback_implicit_task} callback 
with \code{ompt_scope_begin} as its \plc{endpoint} argument for each occurrence 
of an \plc{implicit-task-begin} event in that thread. Similarly, A thread 
dispatches a registered \code{ompt_callback_implicit_task} callback with 
\code{ompt_scope_end} as its \plc{endpoint} argument for each occurrence 
of an \plc{implicit-task-end} event in that thread. The callback occurs in 
the context of the implicit task. The callback has type signature
\code{ompt_callback_implicit_task_t}.

A thread dispatches a registered \code{ompt_callback_parallel_end}
callback for each occurrence of a \plc{parallel-end} event in that
thread.  The callback occurs in the task that encounters the \code{parallel} 
construct.  This callback has the type signature \code{ompt_callback_parallel_end_t}.

A thread dispatches a registered \code{ompt_callback_thread_begin}
callback for the \plc{thread-begin} event in that thread. The callback 
occurs in the context of the thread.  The callback has type signature
\code{ompt_callback_thread_begin_t}.

A thread dispatches a registered \code{ompt_callback_thread_end}
callback for the \plc{thread-end} event in that thread. The callback 
occurs in the context of the thread.  The callback has type signature
\code{ompt_callback_thread_end_t}.

\restrictions
Restrictions to the \code{parallel} construct are as follows:

\begin{itemize}
\item A program that branches into or out of a \code{parallel} region is 
      non-conforming.
\item A program must not depend on any ordering of the evaluations of the clauses 
      of the \code{parallel} directive, or on any side effects of the evaluations 
      of the clauses.
\item At most one \code{if} clause can appear on the directive.
\item At most one \code{proc_bind} clause can appear on the directive.
\item At most one \code{num_threads} clause can appear on the directive. 
      The \code{num_threads} expression must evaluate to a positive integer value.

\begin{cppspecific}
\item A \code{throw} executed inside a \code{parallel} region must cause execution 
      to resume within the same \code{parallel} region, and the same thread that 
      threw the exception must catch it.
\end{cppspecific}
\end{itemize}

\crossreferences
\begin{itemize}

\item OpenMP execution model, see \specref{sec:Execution Model}.

\item \code{num_threads} clause, see \specref{sec:parallel Construct}.

\item \code{proc_bind} clause, see \specref{subsec:Controlling OpenMP Thread Affinity}.

\item \code{allocate} clause, see
\specref{subsec:allocate Clause}.

\item \code{if} clause, see \specref{sec:if Clause}.

\item \code{default}, \code{shared}, \code{private}, \code{firstprivate}, 
and \code{reduction} clauses, see \specref{subsec:Data-Sharing Attribute Clauses}.

\item \code{copyin} clause, see
\specref{subsec:Data Copying Clauses}.

\item \code{omp_get_thread_num} routine, see
\specref{subsec:omp_get_thread_num}.

\item \code{ompt_scope_begin} and \code{ompt_scope_end}, see
  \specref{sec:ompt_scope_endpoint_t}.

\item \code{ompt_callback_thread_begin_t}, see
  \specref{sec:ompt_callback_thread_begin_t}.

\item \code{ompt_callback_thread_end_t}, see
  \specref{sec:ompt_callback_thread_end_t}.

\item \code{ompt_callback_parallel_begin_t}, see
  \specref{sec:ompt_callback_parallel_begin_t}.

\item \code{ompt_callback_parallel_end_t},
  see \specref{sec:ompt_callback_parallel_end_t}.

\item \code{ompt_callback_implicit_task_t}, see
  \specref{sec:ompt_callback_implicit_task_t}.

\end{itemize}











\subsection{Determining the Number of Threads for a \hcode{parallel} Region}
\label{subsec:Determining the Number of Threads for a parallel Region}
When execution encounters a \code{parallel} directive, the value of the \code{if} 
clause or \code{num_threads} clause (if any) on the directive, the current parallel 
context, and the values of the \plc{nthreads-var}, \plc{dyn-var}, 
\plc{thread-limit-var}, \plc{max-active-levels-var}, and \plc{nest-var}
ICVs are used to determine the number of threads to use in the region.

Using a variable in an \code{if} or \code{num_threads} clause expression of a
\code{parallel} construct causes an implicit reference to the variable in all 
enclosing constructs. The \code{if} clause expression and the \code{num_threads} 
clause expression are evaluated in the context outside of the \code{parallel} 
construct, and no ordering of those evaluations is specified. In what order or 
how many times any side effects of the evaluation of the \code{num_threads} or 
\code{if} clause expressions occur is also unspecified.

When a thread encounters a \code{parallel} construct, the number of threads is 
determined according to Algorithm 2.1.



\begin{samepage}
\nolinenumbers\line(1,0){400}\\[.4\baselineskip]
\hspace{1.6in}{\Large \textbf{Algorithm 2.1}}\\[-0.5\baselineskip]
\line(1,0){400}\linenumbers

\begin{quote}
\textbf{let} \plc{ThreadsBusy} be the number of OpenMP threads currently executing in
this contention group;

\textbf{let} \plc{ActiveParRegions} be the number of enclosing active parallel regions;

\textbf{if} an \code{if} clause exists

\textbf{then let} \plc{IfClauseValue} be the value of the \code{if} clause expression;

\textbf{else let} \plc{IfClauseValue} = \plc{true};

\textbf{if} a \code{num_threads} clause exists

\textbf{then let} \plc{ThreadsRequested} be the value of the \code{num_threads} clause
expression;

\textbf{else let} \plc{ThreadsRequested} = value of the first element of \plc{nthreads-var};

\textbf{let} \plc{ThreadsAvailable} = (\plc{thread-limit-var} - \plc{ThreadsBusy} + 1);

\textbf{if} (\plc{IfClauseValue} = \plc{false})

\textbf{then} number of threads = 1;

\textbf{else if} (\plc{ActiveParRegions} >= 1) \textbf{and} (\plc{nest-var} = \plc{false})

\textbf{then} number of threads = 1;

\textbf{else if} (\plc{ActiveParRegions} = \plc{max-active-levels-var})

\textbf{then} number of threads = 1;

\textbf{else if} (\plc{dyn-var} = \plc{true}) \textbf{and} (\plc{ThreadsRequested} <= \plc{ThreadsAvailable})

\textbf{then} number of threads = [ 1 : \plc{ThreadsRequested} ];

\textbf{else if} (\plc{dyn-var} = \plc{true}) \textbf{and} (\plc{ThreadsRequested} > \plc{ThreadsAvailable})

\textbf{then} number of threads = [ 1 : \plc{ThreadsAvailable} ];

\textbf{else if} (\plc{dyn-var} = \plc{false}) \textbf{and} (\plc{ThreadsRequested} <= \plc{ThreadsAvailable})

\textbf{then} number of threads = \plc{ThreadsRequested};

\textbf{else if} (\plc{dyn-var} = \plc{false}) \textbf{and} (\plc{ThreadsRequested} > \plc{ThreadsAvailable})

\textbf{then} behavior is implementation defined;
\end{quote}

\nolinenumbers\line(1,0){400}\linenumbers
\end{samepage}
\bigskip

\begin{note}
Since the initial value of the \plc{dyn-var} ICV is implementation defined, programs
that depend on a specific number of threads for correct execution should explicitly
disable dynamic adjustment of the number of threads.
\end{note}

\crossreferences
\begin{itemize}

\item \code{parallel} construct, see \specref{sec:parallel Construct}.

\item \plc{nthreads-var}, \plc{dyn-var}, \plc{thread-limit-var},
\plc{max-active-levels-var}, and \plc{nest-var} ICVs, see
\specref{sec:Internal Control Variables}.

\item \code{if} clause, see \specref{sec:if Clause}.

\item \code{num_threads} clause, see \specref{sec:parallel Construct}.
\end{itemize}










\subsection{Controlling OpenMP Thread Affinity}
\label{subsec:Controlling OpenMP Thread Affinity}
\index{controlling OpenMP thread affinity}
\index{thread affinity}
\index{affinity}

When a thread encounters a \code{parallel} directive without a \code{proc_bind} clause, the \plc{bind-var} ICV is used to determine the policy for assigning OpenMP threads to places within the current place partition, that is, the places listed in the \plc{place-partition-var} ICV for the implicit task of the encountering thread. If the \code{parallel} directive has a \code{proc_bind} clause then the binding policy specified by the \code{proc_bind} clause overrides the policy specified by the first element of the \plc{bind-var} ICV. Once a thread in the team is assigned to a place, the OpenMP implementation should not move it to another place.

The \code{master} thread affinity policy instructs the execution environment to assign every thread in the team to the same place as the master thread. The place partition is not changed by this policy, and each implicit task inherits the \plc{place-partition-var} ICV of the parent implicit task.

The \code{close} thread affinity policy instructs the execution environment to assign the threads in the team to places close to the place of the parent thread. The place partition is not changed by this policy, and each implicit task inherits the \plc{place-partition-var} ICV of the parent implicit task. If $T$ is the number of threads in the team, and $P$ is the number of places in the parent's place partition, then the assignment of threads in the team to places is as follows:

\begin{itemize}
\item $T\leq P$.
The master thread executes on the place of the parent thread. The thread with the next smallest thread number executes on the next place in the place partition, and so on, with wrap around with respect to the place partition of the master thread.
\item $T>P$.
Each place $P$ will contain $S_{p}$ threads with consecutive thread numbers,
where $\blfloor T/P \brfloor \leq Sp \leq \blceil T/P \brceil$. The first $S_{0}$ threads (including the master thread) are assigned to the place of the parent thread. The next $S_{1}$ threads are assigned to the next place in the place partition, and so on, with wrap around with respect to the place partition of the master thread. When $P$ does not divide $T$ evenly, the exact number of threads in a particular place is implementation defined.
\end{itemize}


The purpose of the \code{spread} thread affinity policy is to create a sparse distribution for a
team of $T$ threads among the $P$ places of the parent's place partition. A sparse distribution is achieved
by first subdividing the parent partition into $T$ subpartitions if
$T\leq P$, or $P$ subpartitions if $T>P$. Then one thread ($T\leq P$) or a
set of threads ($T>P$) is assigned to each subpartition. The
\plc{place-partition-var} ICV of each implicit task is set to its subpartition.
The subpartitioning is not only a mechanism for achieving a sparse
distribution, it also defines a subset of places for a thread to use when
creating a nested \code{parallel} region. The assignment of threads to places is as
follows:

\begin{itemize}
\item $T\leq P$. The parent thread's place partition is split into $T$ subpartitions, where each subpartition
contains $\blfloor P/T \brfloor$ or $\blceil P/T \brceil$ consecutive places. A single thread is assigned to each subpartition. The master thread executes on the place of the parent thread and is assigned to the subpartition that includes that place. The thread with the next smallest thread number is assigned to the first place in the next subpartition, and so on, with wrap around with respect to the original place partition of the master thread.

\item $T>P$. The parent thread's place partition is split into $P$ subpartitions, each consisting of a single place. Each subpartition is assigned $S_{p}$ threads with consecutive thread numbers, where $\blfloor T/P \brfloor \leq S_{p} \leq \blceil T/P \brceil$. The first $S_{0}$ threads (including the master thread) are assigned to the subpartition containing the place of the parent thread. The next $S_{1}$ threads are assigned to the next subpartition, and so on, with wrap around with respect to the original place partition of the master thread. When P does not divide $T$ evenly, the exact number of threads in a particular subpartition is implementation defined.
\end{itemize}

The determination of whether the affinity request can be fulfilled is implementation defined. If the affinity request cannot be fulfilled, then the affinity of threads in the team is implementation defined.

\begin{note}
Wrap around is needed if the end of a place partition is reached before all thread assignments are done. For example, wrap around may be needed in the case of \code{close} and $T\leq P$, if the master thread is assigned to a place other than the first place in the place partition. In this case, thread 1 is assigned to the place after the place of the master place, thread 2 is assigned to the place after that, and so on. The end of the place partition may be reached before all threads are assigned. In this case, assignment of threads is resumed with the first place in the place partition.
\end{note}

%% \pagebreak
